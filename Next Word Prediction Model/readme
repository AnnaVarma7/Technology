ğŸ§  Next Word Prediction Model using LSTM in Python
This project implements a Next Word Prediction model using LSTM-based neural networks trained on Sherlock Holmes stories. It demonstrates how sequential context in natural language can be used to predict the most probable next word, a fundamental task in NLP applications like messaging apps and search engines.

ğŸ¯ Objective
To build a neural network model that predicts the next word in a sequence, using historical text data as context.

ğŸ§° Tools & Libraries
Python
TensorFlow / Keras
NumPy
Sherlock Holmes plain text dataset

ğŸ“ Dataset
Source: sherlock-holm.es_stories_plain-text_advs.txt
Preprocessing: Text cleaned and tokenized into sequences

ğŸ”„ Workflow Summary
1. Tokenization
Text split into tokens using Keras Tokenizer

Vocabulary limited to 10,000 most frequent words

2. Input Sequence Creation
Created n-gram sequences for training (e.g., â€œI willâ€ â†’ â€œleaveâ€)
Padded sequences to uniform length

3. Model Preparation
Input: All tokens except the last
Target: Last token in the sequence (the word to predict)

4. Neural Network Architecture
Embedding â†’ LSTM (150 units) â†’ LSTM (100 units) â†’ Dense (Softmax)
Total parameters: ~1.9 million
Optimizer: adam
Loss Function: sparse_categorical_crossentropy

5. Training
Model trained on padded sequences
One-hot encoding applied to output labels
Training time: ~1 hour (depending on compute)

ğŸ“ˆ Prediction Example
Seed Text:
I will leave if they
Generated Output (3 words):

I will leave if they tilted tilted future
Note: Results may vary based on training duration and text preprocessing.

ğŸ“Œ Key Takeaways
LSTMs are well-suited for capturing long-term dependencies in text.
The model learns to predict contextually relevant next words using training data.
With more training data and epochs, output quality improves.
